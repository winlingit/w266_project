{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a class=\"anchor\" id=\"top\"></a>\n",
    "* [Data Preparation](#Data Prep)\n",
    "* [Entity Resolution](#Entity)\n",
    "* [Relation Extraction](#Relation)\n",
    "* [Query System](#Query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep <a class=\"anchor\" id=\"Data Prep\"></a>\n",
    "[[back to top]](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#standard library imports\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "#modeling functions & utilities\n",
    "from pronounResolution import pronResolution_base, pronResolution_nnMod, pronResolution_nn, pronEval\n",
    "from relationExtract import simpleRE, REEval, getRelations, extract_relation_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "files = [x for x in os.listdir('prep_scripts') if '_gapi' in x]\n",
    "for file in files:\n",
    "    df = pd.read_csv('prep_scripts/' + file)[['speaker']]\n",
    "    print(list(df.speaker.unique()))\n",
    "    print('***')\n",
    "    print('***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to load and annotate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# files = [x for x in os.listdir('prep_scripts') if '_gapi' in x]\n",
    "# df = pd.read_csv('prep_scripts/' + files[1])[['speaker', 'dialogue', 'sentences', 'sentiment', 'entities', 'tokens']]\n",
    "# df['tokens'] = df['tokens'].apply(lambda x: eval(x))\n",
    "# df['sentiment'] = df['sentiment'].apply(lambda x: eval(x))\n",
    "# df['speaker'] = df['speaker'].apply(lambda x: x.strip())\n",
    "# df['entities'] = df['entities'].apply(lambda x: eval(x))\n",
    "# df.head()\n",
    "\n",
    "# returns dataframe with script annotations\n",
    "def loadScript(file_name):\n",
    "    # read file\n",
    "    df = pd.read_csv('prep_scripts/' + file_name)[['speaker', 'dialogue', 'sentences', 'sentiment', 'entities', 'tokens']]\n",
    "\n",
    "    # evaluate strings for lists/dicts of tokens, sentiment, entities\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: eval(x))\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda x: eval(x))\n",
    "    df['speaker'] = df['speaker'].apply(lambda x: x.strip())\n",
    "    df['entities'] = df['entities'].apply(lambda x: eval(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# cList = list(df.speaker.unique())\n",
    "# cCount = Counter(df.speaker)\n",
    "# df['total_sent'] = df['sentiment'].apply(lambda x: x['score'] * x['magnitude'])\n",
    "# cDict = dict(df.groupby('speaker').total_sent.sum())\n",
    "\n",
    "# # number of pronouns for each line\n",
    "# df['num_pron'] = df['tokens'].apply(lambda x: sum([int(t['pos'] == 'PRON') for t in x]))\n",
    "\n",
    "# # total sentiment score for each line\n",
    "# df['total_sent'] = df['sentiment'].apply(lambda x: x['score'] * x['magnitude'])\n",
    "\n",
    "# #set nearby speakers\n",
    "# charRange = 10\n",
    "# nearbyList = np.dstack((df.shift(i).speaker.values for i in range(-charRange, charRange+1)))[0]\n",
    "# df['nearbyChars'] = None\n",
    "# for i, nearbyChars in enumerate(nearbyList):\n",
    "#     df.set_value(i, 'nearbyChars', nearbyChars)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "\n",
    "# enhances annotations with pronoun counts, nearby speakers, and sentiments for each line\n",
    "def annotateScript(df):\n",
    "    \n",
    "    # groups of pronouns\n",
    "    personPron1 = ['i', 'me', 'my', 'mine', 'myself']\n",
    "    personPron1p = ['we', 'us', 'ours', 'our', 'ourselves']\n",
    "    personPron2 = ['you', 'your', 'yours', 'yourself']\n",
    "    personPron3m = ['he', 'his', 'him', 'himself']\n",
    "    personPron3f = ['she', 'her', 'hers', 'herself']\n",
    "    personPron3p = ['they', 'them', 'theirs', 'themselves']\n",
    "    personPron = personPron1 + personPron1p + personPron2 + personPron3m + personPron3f + personPron3p\n",
    "    \n",
    "    # number of pronouns for each line\n",
    "    df['num_pron'] = df['tokens'].apply(lambda x: sum([int((t['pos'] == 'PRON') and (t['content'].lower() in personPron)) for t in x]))\n",
    "\n",
    "    # total sentiment score for each line\n",
    "    df['total_sent'] = df['sentiment'].apply(lambda x: x['score'] * x['magnitude'])\n",
    "\n",
    "    # previous and next speaker for each line\n",
    "    df['speaker_prev'] = df.speaker.shift(1)\n",
    "    df['speaker_next'] = df.speaker.shift(-1)\n",
    "\n",
    "    #set nearby speakers\n",
    "    charRange = 10\n",
    "    nearbyList = np.dstack((df.shift(i).speaker.values for i in range(-charRange, charRange+1)))[0]\n",
    "    df['nearbyChars'] = None\n",
    "    for i, nearbyChars in enumerate(nearbyList):\n",
    "        df.set_value(i, 'nearbyChars', nearbyChars)\n",
    "\n",
    "    return df\n",
    "\n",
    "# selects random lines to evaluate in annotated script with unknown entities (pronouns) resolved\n",
    "def selectEvalLines(df, numExamples):\n",
    "    \n",
    "    # indexes for lines of dialogue with resolved pronouns\n",
    "    pronIndex = list(df[df.num_pron > 0].index)\n",
    "    \n",
    "    # sample random line to evaluate resolved pronoun\n",
    "    evalLines = np.random.choice(pronIndex, min(len(pronIndex), numExamples), replace=False)\n",
    "    \n",
    "    return evalLines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View files for annotated movie scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get files for annotated scripts\n",
    "files = [x for x in os.listdir('prep_scripts') if '_gapi.csv' in x]\n",
    "\n",
    "print 'annotated scripts:'\n",
    "for i, f in enumerate(files):\n",
    "    print i, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load set of raw annotated scripts and add annotations/features for speakers, sentiment, and pronouns.  Select lines to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list of file indexes for Avengers (1,11) and X-Men movies (15-19)\n",
    "fileIndex = [1, 11, 15, 16, 18]\n",
    "\n",
    "# dict to hold name, annotations, characters, and other info for scripts\n",
    "scripts = defaultdict(lambda: defaultdict())\n",
    "\n",
    "for i in fileIndex:\n",
    "    # load annotated script\n",
    "    df = loadScript(files[i])\n",
    "    \n",
    "    # add features to annotated script\n",
    "    df = annotateScript(df)\n",
    "    \n",
    "    # list of unique characters, mentions, overall sentiment\n",
    "    cCount = Counter(df.speaker)\n",
    "    \n",
    "    # script name for printing\n",
    "    scripts[i]['name'] = files[i]\n",
    "    \n",
    "    # annotated script data\n",
    "    scripts[i]['df'] = df\n",
    "    \n",
    "    # unique characters and counts in script\n",
    "    scripts[i]['chars'] = cCount\n",
    "    \n",
    "    # lines to evaluate for each script\n",
    "    scripts[i]['eval'] = selectEvalLines(scripts[i]['df'], numExamples=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Entity Resolution <a class=\"anchor\" id=\"Entity\"></a>\n",
    "[[back to top]](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Base Model (pronResolution_base): sets reference as random character from script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### copy scripts\n",
    "scripts0 = scripts.copy()\n",
    "\n",
    "# apply model to all scripts\n",
    "for i in fileIndex:\n",
    "    charList = scripts0[i]['chars'].keys()\n",
    "    scripts0[i]['df'].apply(lambda x: pronResolution_base(charList, x), axis=1)\n",
    "    \n",
    "# manually evaluate results for all scripts\n",
    "pronEval(scripts0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Nearest Speaker Model (pronResolution_nn)\n",
    "* sets entity for first-person pronouns to speaker\n",
    "* sets entity for second-person pronouns to random choice between previous and next speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# copy scripts\n",
    "scripts1 = scripts.copy()\n",
    "\n",
    "# apply model to all scripts\n",
    "for i in fileIndex:\n",
    "    charList = scripts1[i]['chars'].keys()\n",
    "    scripts1[i]['df'].apply(lambda x: pronResolution_nn(charList, x), axis=1)\n",
    "    \n",
    "# manually evaluate results for all scripts\n",
    "pronEval(scripts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Probability-Weighted Nearby Entities (pronResolution_nnMod):\n",
    "* Set entity for first-person pronouns to speaker\n",
    "* Set entity for second- and third-person pronouns to entity based on distribution of person entities in nearby characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# copy scripts\n",
    "scripts2 = scripts.copy()\n",
    "\n",
    "# apply model to all scripts\n",
    "for i in fileIndex:\n",
    "    charCounter = scripts2[i]['chars'] \n",
    "    scripts2[i]['df'].apply(lambda x: pronResolution_nnMod(charCounter, x, absolute=False), axis=1)\n",
    "    \n",
    "# manually evaluate results for all scripts\n",
    "pronEval(scripts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write dfs with pronoun references added\n",
    "for fileName in files:\n",
    "    df = loadScript(f)\n",
    "    df = annotateScript(df)\n",
    "    charCounter = Counter(df['speaker'])\n",
    "    df.apply(lambda x: pronResolution_nnMod(charCounter, x, absolute=False), axis=1)\n",
    "    df.to_csv(fileName[:-4] + '_prons_nnMod.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Relation Extraction <a class=\"anchor\" id=\"Relation\"></a>\n",
    "[[back to top]](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.apply(lambda x: pronResolution_nnMod(cCount, x), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['relations'] = df.apply(lambda x:extract_relation_categories(x), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "REEval([df], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Everything Together, a Simple Query System <a class=\"anchor\" id=\"Query\"></a>\n",
    "[[back to top]](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def checkQuery(relationList, ent1, ent2, relationClass):\n",
    "    for relation in relationList:\n",
    "        if ent1 in relation['ent1'] and ent2 in relation['ent2'] and relationClass == relation['class']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def printAnswer(row):\n",
    "    print('Movie: {}, Line {}'.format(row.movie, row.lineNum))\n",
    "    print('{}: {}'.format(row.speaker, row.dialogue))\n",
    "    print()\n",
    "    \n",
    "def queryScore(relationList, query, relationClass):\n",
    "    querySet = set(query.split(' '))\n",
    "    resultScore = 0\n",
    "    \n",
    "    for relation in relationList:\n",
    "        relationSet = set()\n",
    "        if type(relation['ent1']) == str:\n",
    "            relationSet |= set(relation['ent1'].lower().split())\n",
    "        else:\n",
    "            for ent in relation['ent1']:\n",
    "                #print(set(ent.split()))\n",
    "                relationSet |= set(ent.lower().split())\n",
    "            \n",
    "        if type(relation['ent2']) == str:\n",
    "            #print(relation['ent2'])\n",
    "            relationSet |= set(relation['ent2'].lower().split())\n",
    "        else:\n",
    "            for ent in relation['ent2']:\n",
    "                relationSet |= set(ent.lower().split())\n",
    "        \n",
    "        relationSet |= set(relation['relation'].lower().split())\n",
    "        relationSet |= set(relationClass[relation['class']].lower().split())\n",
    "        tempScore = len(relationSet & querySet) / (len(relationSet) + len(querySet))\n",
    "        \n",
    "        if tempScore > resultScore:\n",
    "            resultScore = tempScore\n",
    "        \n",
    "    return resultScore\n",
    "\n",
    "#Simple Query System\n",
    "\n",
    "print('Select the movies of your interest:')\n",
    "print('***Enter all to use all movies')\n",
    "print('***Enter n, m, x, y (numbers separated by commas) for specific selections')\n",
    "print('***Enter random, n for n random selections\\n')\n",
    "\n",
    "files = [x for x in os.listdir('prep_scripts') if '_gapi' in x]\n",
    "for i, fileName in enumerate(files):\n",
    "    print('{}. {}'.format(i+1, re.split(r'_tw_|_imsdb_', fileName)[0]))\n",
    "\n",
    "\n",
    "x = input()\n",
    "\n",
    "\n",
    "#random selection\n",
    "try:\n",
    "    if 'random' in x:\n",
    "        queryFiles = np.random.choice(files, int(x.split(',')[-1]), replace=False)\n",
    "    elif x != 'all':\n",
    "        queryFiles = np.array(files)[[int(select) - 1 for select in x.split(',')]]\n",
    "    #use all files\n",
    "    else:\n",
    "        queryFiles = files    \n",
    "        \n",
    "except:\n",
    "    print('\\nunexpected input, will use all movie files\\n')\n",
    "    queryFiles = files    \n",
    "\n",
    "#print(queryFiles)\n",
    "df_data = None\n",
    "charSet = set()\n",
    "\n",
    "for i, fileName in enumerate(queryFiles):    \n",
    "    df = pd.read_csv('prep_scripts/'+fileName)[['speaker', 'dialogue', 'sentences', 'sentiment', 'entities', 'tokens']]\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: eval(x))\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda x: eval(x))\n",
    "    df['total_sent'] = df['sentiment'].apply(lambda x: x['score'] * x['magnitude'])\n",
    "    df['entities'] = df['entities'].apply(lambda x: eval(x))\n",
    "    df['movie'] = re.split(r'_tw_|_imsdb_', fileName)[0]\n",
    "    df['lineNum'] = df.index + 1\n",
    "    \n",
    "    charRange = 10\n",
    "    nearbyList = np.dstack((df.shift(i).speaker.values for i in range(-charRange, charRange+1)))[0]\n",
    "    df['nearbyChars'] = None\n",
    "    for line, nearbyChars in enumerate(nearbyList):\n",
    "        df.set_value(line, 'nearbyChars', nearbyChars)\n",
    "    \n",
    "    cList = list(df.speaker.unique())\n",
    "    cDict = dict(df.groupby('speaker').total_sent.sum())\n",
    "    \n",
    "    #resolve entities\n",
    "    df.apply(lambda x:pronResolution_nnMod(cList, x), axis=1)\n",
    "    \n",
    "    #extract relations\n",
    "    df['relations'] = df.apply(lambda x:extract_relation_categories(x), axis=1)\n",
    "    \n",
    "    if i == 0:\n",
    "        df_data = df[df.relations.notnull()]        \n",
    "        \n",
    "    else:\n",
    "        df_data = pd.concat((df_data, df[df.relations.notnull()]))\n",
    "    \n",
    "    charSet |= set(df.speaker.unique())\n",
    "\n",
    "relationClasses = getRelations()\n",
    "    \n",
    "print('Type end to finish at any time')\n",
    "print('Choose one of the following:')\n",
    "print('1. Structured search')\n",
    "print('2. Free form query')\n",
    "searchType = int(input()) - 1\n",
    "\n",
    "#relationList = df_data[df_data.hasRelation == True]['relations'].values\n",
    "\n",
    "if not searchType:\n",
    "    \n",
    "    while True:\n",
    "        print('Characters: ')\n",
    "        print(charSet)\n",
    "        print('\\nRelations:')\n",
    "        for k, v in relationClasses.items():\n",
    "            print('{}. {}'.format(k+1, v))\n",
    "        print('What relation are you looking for?')\n",
    "        ent1 = input('Entity 1:')\n",
    "        if ent1 == 'end':\n",
    "            break\n",
    "        ent2 = input('Entity 2:')\n",
    "        if ent2 == 'end':\n",
    "            break\n",
    "        relationClass = int(input('Relation category: '))-1\n",
    "\n",
    "        qMatch = df_data.relations.apply(lambda x: checkQuery(x, ent1, ent2, relationClass))\n",
    "        if sum(qMatch) == 0:\n",
    "            print('nothing found\\n')\n",
    "        else:\n",
    "            df_data[qMatch].apply(lambda x: printAnswer(x), axis=1)\n",
    "\n",
    "else:\n",
    "    while True:\n",
    "        query = input('Enter query')\n",
    "        if query == 'end':\n",
    "            break\n",
    "        df = df_data.copy()\n",
    "        df['queryScore'] = df.relations.apply(lambda x: queryScore(x, query, relationClasses))\n",
    "        df = df.sort_values(by='queryScore', ascending=False).head().copy()\n",
    "        df.apply(lambda x: printAnswer(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
